{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "23598b20-c31c-4c70-aea7-022dca339383",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from textblob import TextBlob\n",
    "import spacy\n",
    "import logging\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "import xlsxwriter\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO, \n",
    "                    format='%(asctime)s - %(levelname)s: %(message)s',\n",
    "                    filename='sentiment_analysis.log')\n",
    "\n",
    "# Download necessary NLTK resources\n",
    "try:\n",
    "    nltk.download('punkt', quiet=True)\n",
    "    nltk.download('vader_lexicon', quiet=True)\n",
    "except Exception as e:\n",
    "    logging.error(f\"Error downloading NLTK resources: {e}\")\n",
    "\n",
    "# Load spaCy English model\n",
    "try:\n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "except Exception as e:\n",
    "    logging.error(f\"Error loading spaCy model: {e}\")\n",
    "    print(\"Please install spaCy English model with: python -m spacy download en_core_web_sm\")\n",
    "    nlp = None\n",
    "\n",
    "# Ensure input and output folders exist\n",
    "os.makedirs('input', exist_ok=True)\n",
    "os.makedirs('output', exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e3a28d84-7e43-44da-b6a2-bd3fb545464b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data from CSV\n",
    "def load_lyrics_data(input_file):\n",
    "    \"\"\"\n",
    "    Load lyrics data from a CSV file\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Try reading with different encodings\n",
    "        encodings = ['utf-8', 'latin-1', 'iso-8859-1']\n",
    "        \n",
    "        for encoding in encodings:\n",
    "            try:\n",
    "                df = pd.read_csv(input_file, encoding=encoding)\n",
    "                print(f\"Successfully read file using {encoding} encoding\")\n",
    "                return df\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to read with {encoding} encoding: {e}\")\n",
    "        \n",
    "        raise ValueError(\"Could not read the file with any of the attempted encodings\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error reading file: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5bbf24e3-4539-4098-961a-914df81ae731",
   "metadata": {},
   "outputs": [],
   "source": [
    "def advanced_sentiment_analysis(text):\n",
    "    \"\"\"\n",
    "    Advanced sentiment analysis using multiple techniques\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str) or text.strip() == '':\n",
    "        return {\n",
    "            'sentiment': 'Neutral',\n",
    "            'sentiment_score': 0,\n",
    "            'positive_prob': 0,\n",
    "            'negative_prob': 0,\n",
    "            'debug_info': 'Empty text'\n",
    "        }\n",
    "    \n",
    "    # Clean text\n",
    "    cleaned_text = advanced_clean_text(text)\n",
    "    \n",
    "    # Multiple sentiment analysis techniques\n",
    "    results = []\n",
    "    debug_info = {}\n",
    "    \n",
    "    # 1. TextBlob Sentiment\n",
    "    try:\n",
    "        blob_sentiment = TextBlob(cleaned_text)\n",
    "        text_blob_score = blob_sentiment.sentiment.polarity\n",
    "        results.append(text_blob_score)\n",
    "        debug_info['textblob_score'] = text_blob_score\n",
    "    except Exception as e:\n",
    "        logging.warning(f\"TextBlob sentiment error: {e}\")\n",
    "        text_blob_score = 0\n",
    "    \n",
    "    # 2. NLTK VADER Sentiment Intensity Analyzer\n",
    "    try:\n",
    "        sia = SentimentIntensityAnalyzer()\n",
    "        vader_scores = sia.polarity_scores(cleaned_text)\n",
    "        vader_score = vader_scores['compound']\n",
    "        results.append(vader_score)\n",
    "        debug_info['vader_scores'] = vader_scores\n",
    "    except Exception as e:\n",
    "        logging.warning(f\"VADER sentiment error: {e}\")\n",
    "        vader_score = 0\n",
    "    \n",
    "    # 3. SpaCy Named Entity and Dependency Analysis (if model loaded)\n",
    "    spacy_score = 0\n",
    "    if nlp:\n",
    "        try:\n",
    "            doc = nlp(cleaned_text)\n",
    "            # Simple heuristic based on entities and dependency\n",
    "            entity_sentiment = sum(1 if ent.label_ in ['ORG', 'PERSON'] else 0 for ent in doc.ents)\n",
    "            spacy_score = entity_sentiment * 0.1\n",
    "            debug_info['spacy_entities'] = [ent.text for ent in doc.ents]\n",
    "        except Exception as e:\n",
    "            logging.warning(f\"SpaCy analysis error: {e}\")\n",
    "    \n",
    "    # Combine scores with weighted average\n",
    "    combined_score = np.mean(results + [spacy_score]) if results else 0\n",
    "    \n",
    "    # Sentiment categorization with more nuanced thresholds\n",
    "    if combined_score > 0.2:\n",
    "        sentiment = 'Positive'\n",
    "    elif combined_score < -0.2:\n",
    "        sentiment = 'Negative'\n",
    "    else:\n",
    "        sentiment = 'Neutral'\n",
    "    \n",
    "    return {\n",
    "        'sentiment': sentiment,\n",
    "        'sentiment_score': combined_score,\n",
    "        'positive_prob': max(combined_score, 0),\n",
    "        'negative_prob': abs(min(combined_score, 0)),\n",
    "        'debug_info': debug_info\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "54f91313-d57c-48ac-85d9-4a6db7edf5f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_lyrics_file(file_path):\n",
    "    \"\"\"\n",
    "    Process individual lyrics file\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Extract artist and title from filename\n",
    "        filename = os.path.basename(file_path)\n",
    "        artist, title = filename.rsplit('-', 1)\n",
    "        title = title.replace('.txt', '')\n",
    "        \n",
    "        # Read lyrics\n",
    "        with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "            lyrics = f.read()\n",
    "        \n",
    "        # Perform sentiment analysis\n",
    "        sentiment_result = advanced_sentiment_analysis(lyrics)\n",
    "        \n",
    "        # Create a DataFrame with the results\n",
    "        result_df = pd.DataFrame({\n",
    "            'Artist': [artist],\n",
    "            'Title': [title],\n",
    "            'Lyrics': [lyrics],\n",
    "            'Sentiment': [sentiment_result['sentiment']],\n",
    "            'Sentiment Score': [sentiment_result['sentiment_score']],\n",
    "            'Positive Probability': [sentiment_result['positive_prob']],\n",
    "            'Negative Probability': [sentiment_result['negative_prob']]\n",
    "        })\n",
    "        \n",
    "        logging.info(f\"Processed {filename}: {sentiment_result['sentiment']} sentiment\")\n",
    "        \n",
    "        return result_df, sentiment_result['debug_info']\n",
    "    \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error processing {file_path}: {e}\")\n",
    "        return None, None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1ba33053-8e0a-45ad-a2f9-d3639b22b8e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_bulk_sentiment_analysis():\n",
    "    \"\"\"\n",
    "    Perform sentiment analysis on all text files in input folder\n",
    "    \"\"\"\n",
    "    # Find text files in the input folder\n",
    "    input_files = [f for f in os.listdir('input') if f.endswith('.txt')]\n",
    "    \n",
    "    if not input_files:\n",
    "        logging.warning(\"No text files found in the input folder.\")\n",
    "        return\n",
    "    \n",
    "    # Prepare list to store results\n",
    "    all_results = []\n",
    "    debug_logs = {}\n",
    "    \n",
    "    # Process each input file\n",
    "    for input_file in input_files:\n",
    "        full_path = os.path.join('input', input_file)\n",
    "        print(f\"Processing file: {input_file}\")\n",
    "        \n",
    "        # Process individual lyrics file\n",
    "        result_df, debug_info = process_lyrics_file(full_path)\n",
    "        \n",
    "        if result_df is not None:\n",
    "            all_results.append(result_df)\n",
    "            debug_logs[input_file] = debug_info\n",
    "    \n",
    "    # Combine all results\n",
    "    if all_results:\n",
    "        final_df = pd.concat(all_results, ignore_index=True)\n",
    "        \n",
    "        # Export to Excel with multiple sheets\n",
    "        output_file = os.path.join('output', 'lyrics_sentiment_analysis.xlsx')\n",
    "        \n",
    "        with pd.ExcelWriter(output_file, engine='xlsxwriter') as writer:\n",
    "            # Full Results Sheet\n",
    "            final_df.to_excel(writer, sheet_name='Full Results', index=False)\n",
    "            \n",
    "            # Individual Sheets for each song\n",
    "            for index, row in final_df.iterrows():\n",
    "                sheet_name = f\"{row['Artist']}_{row['Title']}\"[:31]  # Excel sheet name limit\n",
    "                pd.DataFrame([row]).to_excel(writer, sheet_name=sheet_name, index=False)\n",
    "            \n",
    "            # Sentiment Summary Sheet\n",
    "            sentiment_summary = final_df['Sentiment'].value_counts(normalize=True) * 100\n",
    "            sentiment_summary.to_excel(writer, sheet_name='Sentiment Summary')\n",
    "        \n",
    "        # Create visualizations\n",
    "        create_visualizations(final_df)\n",
    "        \n",
    "        # Save debug information\n",
    "        with open('output/debug_logs.txt', 'w') as f:\n",
    "            for filename, debug_info in debug_logs.items():\n",
    "                f.write(f\"Filename: {filename}\\n\")\n",
    "                f.write(f\"Debug Info: {debug_info}\\n\\n\")\n",
    "        \n",
    "        print(f\"Analysis complete. Results exported to {output_file}\")\n",
    "    else:\n",
    "        logging.warning(\"No results to export.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "36f66833-853e-4257-b2ee-72c43f04739f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_visualizations(df):\n",
    "    \"\"\"\n",
    "    Create visualizations for sentiment analysis\n",
    "    \"\"\"\n",
    "    output_folder = 'output'\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    \n",
    "    # Create a PDF with multiple visualizations\n",
    "    from matplotlib.backends.backend_pdf import PdfPages\n",
    "    \n",
    "    with PdfPages(os.path.join(output_folder, 'sentiment_visualizations.pdf')) as pdf:\n",
    "        # Sentiment Distribution Pie Chart\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.subplot(1, 2, 1)\n",
    "        sentiment_counts = df['Sentiment'].value_counts()\n",
    "        plt.pie(sentiment_counts, labels=sentiment_counts.index, autopct='%1.1f%%')\n",
    "        plt.title('Sentiment Distribution')\n",
    "        \n",
    "        # Sentiment Score Box Plot\n",
    "        plt.subplot(1, 2, 2)\n",
    "        sns.boxplot(x='Sentiment', y='Sentiment Score', data=df)\n",
    "        plt.title('Sentiment Scores Distribution')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        pdf.savefig()\n",
    "        plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "857cafe8-969e-43d7-a05c-2dc3fa4f55bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: bartjhe-satudalamcinta.txt\n",
      "Processing file: lastchild-duka.txt\n",
      "Analysis complete. Results exported to output\\lyrics_sentiment_analysis.xlsx\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    # Perform bulk sentiment analysis\n",
    "    perform_bulk_sentiment_analysis()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2710a275-6537-4791-9fd6-ca4f68bcd34e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_results(df, output_folder='output'):\n",
    "    \"\"\"\n",
    "    Export analysis results\n",
    "    \"\"\"\n",
    "    # Ensure output folder exists\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    \n",
    "    # Export full results\n",
    "    full_results_path = os.path.join(output_folder, 'lyrics_sentiment_analysis.csv')\n",
    "    df.to_csv(full_results_path, index=False)\n",
    "    print(f\"Full results exported to {full_results_path}\")\n",
    "    \n",
    "    # Export summary\n",
    "    summary_path = os.path.join(output_folder, 'sentiment_summary.txt')\n",
    "    with open(summary_path, 'w', encoding='utf-8') as f:\n",
    "        # Overall sentiment distribution\n",
    "        sentiment_counts = df['sentiment'].value_counts()\n",
    "        f.write(\"Sentiment Distribution:\\n\")\n",
    "        for sentiment, count in sentiment_counts.items():\n",
    "            f.write(f\"{sentiment}: {count} ({count/len(df)*100:.2f}%)\\n\")\n",
    "        \n",
    "        # Average sentiment by artist\n",
    "        f.write(\"\\nAverage Sentiment by Artist:\\n\")\n",
    "        artist_sentiment = df.groupby('artist')['sentiment_score'].mean().sort_values(ascending=False)\n",
    "        for artist, score in artist_sentiment.items():\n",
    "            f.write(f\"{artist}: {score:.4f}\\n\")\n",
    "    \n",
    "    print(f\"Summary exported to {summary_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f9e347a8-93e5-4b5e-95df-a22093ac54ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 2 lyrics\n",
      "Model Evaluation Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Neutral       1.00      1.00      1.00         1\n",
      "\n",
      "    accuracy                           1.00         1\n",
      "   macro avg       1.00      1.00      1.00         1\n",
      "weighted avg       1.00      1.00      1.00         1\n",
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "create_visualizations() missing 1 required positional argument: 'text_columns'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[38], line 29\u001b[0m\n\u001b[0;32m     26\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m labeled_df, model, vectorizer\n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m# Run the main workflow\u001b[39;00m\n\u001b[1;32m---> 29\u001b[0m results_df, sentiment_model, lyrics_vectorizer \u001b[38;5;241m=\u001b[39m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[38], line 21\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     18\u001b[0m model, vectorizer \u001b[38;5;241m=\u001b[39m train_sentiment_model(labeled_df)\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# 5. Create Visualizations\u001b[39;00m\n\u001b[1;32m---> 21\u001b[0m \u001b[43mcreate_visualizations\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabeled_df\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# 6. Export Results\u001b[39;00m\n\u001b[0;32m     24\u001b[0m export_results(labeled_df)\n",
      "\u001b[1;31mTypeError\u001b[0m: create_visualizations() missing 1 required positional argument: 'text_columns'"
     ]
    }
   ],
   "source": [
    "# Main Workflow\n",
    "def main():\n",
    "    # 1. Load Lyrics from Files\n",
    "    try:\n",
    "        lyrics_df = load_lyrics_from_files()\n",
    "        print(f\"Loaded {len(lyrics_df)} lyrics\")\n",
    "    except ValueError as e:\n",
    "        print(e)\n",
    "        return\n",
    "    \n",
    "    # 2. Clean Data\n",
    "    cleaned_df = clean_lyrics(lyrics_df)\n",
    "    \n",
    "    # 3. Label Sentiment\n",
    "    labeled_df = label_sentiment(cleaned_df)\n",
    "    \n",
    "    # 4. Train Model\n",
    "    model, vectorizer = train_sentiment_model(labeled_df)\n",
    "    \n",
    "    # 5. Create Visualizations\n",
    "    create_visualizations(labeled_df)\n",
    "    \n",
    "    # 6. Export Results\n",
    "    export_results(labeled_df)\n",
    "    \n",
    "    return labeled_df, model, vectorizer\n",
    "\n",
    "# Run the main workflow\n",
    "results_df, sentiment_model, lyrics_vectorizer = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2df86ac8-f6a7-4a82-a016-a35e1120b27e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to analyze a single lyric file\n",
    "def analyze_single_lyric(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        lyrics = file.read().strip()\n",
    "    \n",
    "    sentiment, probabilities = analyze_lyrics_sentiment(lyrics, sentiment_model, lyrics_vectorizer)\n",
    "    \n",
    "    print(f\"Lyrics: {lyrics[:100]}...\")  # First 100 characters\n",
    "    print(f\"Predicted Sentiment: {sentiment}\")\n",
    "    print(f\"Sentiment Probabilities: {probabilities}\")\n",
    "\n",
    "# Uncomment and modify to test a specific lyric file\n",
    "# analyze_single_lyric('input/YourArtist - YourSong.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f644f6b-fc1e-4501-85af-f093815bcb40",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
