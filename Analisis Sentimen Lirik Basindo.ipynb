{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b48865b8-202e-4a97-9a5c-1bd46d8aaf6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: bartjhe-satudalamcinta.txt\n",
      "Processing file: Febri Putri-Runtuh.txt\n",
      "Processing file: Jrock-Ceria.txt\n",
      "Processing file: lastchild-duka.txt\n",
      "Processing file: Project Pop-Ingatlah Hari Ini.txt\n",
      "Analysis complete for bartjhe - satudalamcinta. Results exported to output\\bartjhe-satudalamcinta_sentiment_analysis.xlsx\n",
      "Analysis complete for Febri Putri - Runtuh. Results exported to output\\Febri Putri-Runtuh_sentiment_analysis.xlsx\n",
      "Analysis complete for Jrock - Ceria. Results exported to output\\Jrock-Ceria_sentiment_analysis.xlsx\n",
      "Analysis complete for lastchild - duka. Results exported to output\\lastchild-duka_sentiment_analysis.xlsx\n",
      "Analysis complete for Project Pop - Ingatlah Hari Ini. Results exported to output\\Project Pop-Ingatlah Hari Ini_sentiment_analysis.xlsx\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import json\n",
    "import logging\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Attempt to import Sastrawi with error handling\n",
    "try:\n",
    "    from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "    from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory\n",
    "except ImportError:\n",
    "    print(\"Sastrawi library not found. Please install it using: pip install Sastrawi\")\n",
    "    StemmerFactory = None\n",
    "    StopWordRemoverFactory = None\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO, \n",
    "                    format='%(asctime)s - %(levelname)s: %(message)s',\n",
    "                    filename='indonesian_sentiment_analysis.log')\n",
    "\n",
    "class IndonesianSentimentAnalyzer:\n",
    "    def __init__(self, lexicon_path='leksikon'):\n",
    "        \"\"\"\n",
    "        Initialize sentiment analyzer with Indonesian lexicon data and Sastrawi preprocessors\n",
    "        \"\"\"\n",
    "        # Check if Sastrawi is available\n",
    "        if StemmerFactory is None or StopWordRemoverFactory is None:\n",
    "            raise ImportError(\"Sastrawi library is required but not installed.\")\n",
    "        \n",
    "        self.lexicon_path = lexicon_path\n",
    "        self.positive_words = set()\n",
    "        self.negative_words = set()\n",
    "        self.booster_words = set()\n",
    "        self.negation_words = set()\n",
    "        \n",
    "        # Initialize Sastrawi preprocessors\n",
    "        stemmer_factory = StopWordRemoverFactory()\n",
    "        self.stop_words = set(stemmer_factory.get_stop_words())\n",
    "        \n",
    "        stemmer_factory = StemmerFactory()\n",
    "        self.stemmer = stemmer_factory.create_stemmer()\n",
    "        \n",
    "        self.load_lexicons()\n",
    "    \n",
    "    def load_lexicons(self):\n",
    "        \"\"\"\n",
    "        Load various Indonesian lexicon files\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Load positive words\n",
    "            pos_files = [\n",
    "                os.path.join(self.lexicon_path, 'inset', 'positive.tsv'),\n",
    "                os.path.join(self.lexicon_path, 'inset', '_json_inset-pos.txt')\n",
    "            ]\n",
    "            for pos_file in pos_files:\n",
    "                if os.path.exists(pos_file):\n",
    "                    with open(pos_file, 'r', encoding='utf-8') as f:\n",
    "                        self.positive_words.update(self._read_lexicon_file(f))\n",
    "            \n",
    "            # Load negative words\n",
    "            neg_files = [\n",
    "                os.path.join(self.lexicon_path, 'inset', 'negative.tsv'),\n",
    "                os.path.join(self.lexicon_path, 'inset', '_json_inset-neg.txt')\n",
    "            ]\n",
    "            for neg_file in neg_files:\n",
    "                if os.path.exists(neg_file):\n",
    "                    with open(neg_file, 'r', encoding='utf-8') as f:\n",
    "                        self.negative_words.update(self._read_lexicon_file(f))\n",
    "            \n",
    "            # Load booster and negation words\n",
    "            booster_file = os.path.join(self.lexicon_path, 'sentistrength_id', 'boosterwords_id.txt')\n",
    "            negation_file = os.path.join(self.lexicon_path, 'sentistrength_id', 'negatingword.txt')\n",
    "            \n",
    "            if os.path.exists(booster_file):\n",
    "                with open(booster_file, 'r', encoding='utf-8') as f:\n",
    "                    self.booster_words = set(line.strip().lower() for line in f if line.strip())\n",
    "            \n",
    "            if os.path.exists(negation_file):\n",
    "                with open(negation_file, 'r', encoding='utf-8') as f:\n",
    "                    self.negation_words = set(line.strip().lower() for line in f if line.strip())\n",
    "            \n",
    "            logging.info(f\"Loaded lexicons: {len(self.positive_words)} positive, {len(self.negative_words)} negative words\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error loading lexicons: {e}\")\n",
    "    \n",
    "    def _read_lexicon_file(self, file):\n",
    "        \"\"\"\n",
    "        Read lexicon file with various formats\n",
    "        \"\"\"\n",
    "        words = set()\n",
    "        for line in file:\n",
    "            line = line.strip().lower()\n",
    "            # Handle TSV and JSON-like formats\n",
    "            if '\\t' in line:\n",
    "                word = line.split('\\t')[0].lower()\n",
    "            elif ':' in line:\n",
    "                try:\n",
    "                    word = json.loads(line.replace(\"'\", '\"')).get('word', '').lower()\n",
    "                except:\n",
    "                    word = line.split(':')[0].lower()\n",
    "            else:\n",
    "                word = line.lower()\n",
    "            \n",
    "            if word:\n",
    "                words.add(word)\n",
    "        return words\n",
    "    \n",
    "    def preprocess_text(self, text):\n",
    "        \"\"\"\n",
    "        Preprocess Indonesian text using Sastrawi\n",
    "        \"\"\"\n",
    "        if not isinstance(text, str):\n",
    "            return []\n",
    "        \n",
    "        # Convert to lowercase\n",
    "        text = text.lower()\n",
    "        \n",
    "        # Remove special characters and numbers\n",
    "        text = re.sub(r'[^a-z\\s]', '', text)\n",
    "        \n",
    "        # Tokenize by splitting on whitespace\n",
    "        words = text.split()\n",
    "        \n",
    "        # Remove stop words\n",
    "        words = [word for word in words if word not in self.stop_words]\n",
    "        \n",
    "        # Stemming\n",
    "        words = [self.stemmer.stem(word) for word in words]\n",
    "        \n",
    "        return words\n",
    "    \n",
    "    def analyze_sentiment(self, text):\n",
    "        \"\"\"\n",
    "        Perform sentiment analysis on Indonesian text\n",
    "        \"\"\"\n",
    "        if not isinstance(text, str) or text.strip() == '':\n",
    "            return {\n",
    "                'sentiment': 'Neutral',\n",
    "                'sentiment_score': 0,\n",
    "                'positive_count': 0,\n",
    "                'negative_count': 0,\n",
    "                'total_words': 0\n",
    "            }\n",
    "        \n",
    "        # Preprocess text\n",
    "        words = self.preprocess_text(text)\n",
    "        \n",
    "        # Count sentiments\n",
    "        positive_count = sum(1 for word in words if word in self.positive_words)\n",
    "        negative_count = sum(1 for word in words if word in self.negative_words)\n",
    "        \n",
    "        # Total words\n",
    "        total_words = len(words)\n",
    "        \n",
    "        # Calculate sentiment score\n",
    "        sentiment_score = (positive_count - negative_count) / (total_words + 1)  # Add 1 to avoid division by zero\n",
    "        \n",
    "        # Categorize sentiment\n",
    "        if sentiment_score > 0.1:\n",
    "            sentiment = 'Positive'\n",
    "        elif sentiment_score < -0.1:\n",
    "            sentiment = 'Negative'\n",
    "        else:\n",
    "            sentiment = 'Neutral'\n",
    "        \n",
    "        return {\n",
    "            'sentiment': sentiment,\n",
    "            'sentiment_score': sentiment_score,\n",
    "            'positive_count': positive_count,\n",
    "            'negative_count': negative_count,\n",
    "            'total_words': total_words\n",
    "        }\n",
    "\n",
    "\n",
    "def process_lyrics_file(file_path, sentiment_analyzer):\n",
    "    \"\"\"\n",
    "    Process lyrics file, analyzing entire lyrics and each line\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Extract artist and title from filename\n",
    "        filename = os.path.basename(file_path)\n",
    "        \n",
    "        # More robust filename parsing\n",
    "        parts = filename.rsplit('-', 1)\n",
    "        if len(parts) < 2:\n",
    "            artist = \"Unknown Artist\"\n",
    "            title = filename.replace('.txt', '')\n",
    "        else:\n",
    "            artist = parts[0]\n",
    "            title = parts[1].replace('.txt', '')\n",
    "        \n",
    "        # Read lyrics\n",
    "        with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "            lyrics_text = f.read()\n",
    "            f.seek(0)  # Reset file pointer\n",
    "            lyrics_lines = f.readlines()\n",
    "        \n",
    "        # Analyze entire lyrics\n",
    "        overall_sentiment = sentiment_analyzer.analyze_sentiment(lyrics_text)\n",
    "        \n",
    "        # Analyze each line\n",
    "        line_results = []\n",
    "        for i, line in enumerate(lyrics_lines, 1):\n",
    "            line = line.strip()\n",
    "            if line:\n",
    "                line_sentiment = sentiment_analyzer.analyze_sentiment(line)\n",
    "                line_results.append({\n",
    "                    'artist': artist,\n",
    "                    'title': title,\n",
    "                    'line_number': i,\n",
    "                    'lyrics_line': line,\n",
    "                    'sentiment': line_sentiment['sentiment'],\n",
    "                    'sentiment_score': line_sentiment['sentiment_score'],\n",
    "                    'positive_words': line_sentiment['positive_count'],\n",
    "                    'negative_words': line_sentiment['negative_count'],\n",
    "                    'total_words': line_sentiment['total_words']\n",
    "                })\n",
    "        \n",
    "        # Ensure DataFrame creation works even with empty results\n",
    "        line_df = pd.DataFrame(line_results) if line_results else pd.DataFrame()\n",
    "        \n",
    "        # Overall lyrics sentiment\n",
    "        overall_result = {\n",
    "            'artist': artist,\n",
    "            'title': title,\n",
    "            'overall_sentiment': overall_sentiment['sentiment'],\n",
    "            'overall_sentiment_score': overall_sentiment['sentiment_score'],\n",
    "            'total_positive_words': overall_sentiment['positive_count'],\n",
    "            'total_negative_words': overall_sentiment['negative_count'],\n",
    "            'total_words': overall_sentiment['total_words']\n",
    "        }\n",
    "        \n",
    "        overall_df = pd.DataFrame([overall_result])\n",
    "        \n",
    "        return {\n",
    "            'line_df': line_df,\n",
    "            'overall_df': overall_df\n",
    "        }\n",
    "    \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error processing {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "def perform_bulk_sentiment_analysis():\n",
    "    \"\"\"\n",
    "    Perform sentiment analysis on all text files in input folder\n",
    "    \"\"\"\n",
    "    # Initialize sentiment analyzer\n",
    "    sentiment_analyzer = IndonesianSentimentAnalyzer()\n",
    "    \n",
    "    # Find text files in the input folder\n",
    "    input_folder = 'input'\n",
    "    os.makedirs(input_folder, exist_ok=True)  # Create input folder if it doesn't exist\n",
    "    \n",
    "    input_files = [f for f in os.listdir(input_folder) if f.endswith('.txt')]\n",
    "    \n",
    "    if not input_files:\n",
    "        logging.warning(\"No text files found in the input folder.\")\n",
    "        return\n",
    "    \n",
    "    # Prepare lists to store results\n",
    "    all_line_results = []\n",
    "    all_overall_results = []\n",
    "    \n",
    "    # Process each input file\n",
    "    for input_file in input_files:\n",
    "        full_path = os.path.join(input_folder, input_file)\n",
    "        print(f\"Processing file: {input_file}\")\n",
    "        \n",
    "        # Process individual lyrics file\n",
    "        result = process_lyrics_file(full_path, sentiment_analyzer)\n",
    "        \n",
    "        if result is not None:\n",
    "            # Only append if DataFrames are not empty\n",
    "            if not result['line_df'].empty:\n",
    "                all_line_results.append(result['line_df'])\n",
    "            if not result['overall_df'].empty:\n",
    "                all_overall_results.append(result['overall_df'])\n",
    "    \n",
    "    # Combine and export results\n",
    "    if all_line_results and all_overall_results:\n",
    "        # Create output directory\n",
    "        output_folder = 'output'\n",
    "        os.makedirs(output_folder, exist_ok=True)\n",
    "        \n",
    "        # Process each lyric separately\n",
    "        for i, (line_df, overall_df) in enumerate(zip(all_line_results, all_overall_results), 1):\n",
    "            # Create output file for each lyric\n",
    "            try:\n",
    "                artist = line_df['artist'].iloc[0]\n",
    "                title = line_df['title'].iloc[0]\n",
    "                output_file = os.path.join(output_folder, f'{artist}-{title}_sentiment_analysis.xlsx')\n",
    "                \n",
    "                # Export to Excel\n",
    "                with pd.ExcelWriter(output_file, engine='xlsxwriter') as writer:\n",
    "                    # Line-by-Line Results Sheet\n",
    "                    line_df.to_excel(writer, sheet_name='Line Results', index=False)\n",
    "                    \n",
    "                    # Overall Sentiment Sheet\n",
    "                    overall_df.to_excel(writer, sheet_name='Overall Sentiment', index=False)\n",
    "                    \n",
    "                    # Sentiment Summary\n",
    "                    summary_df = line_df.groupby('sentiment').size().reset_index(name='Count')\n",
    "                    summary_df['Percentage'] = (summary_df['Count'] / len(line_df) * 100).round(2)\n",
    "                    summary_df.to_excel(writer, sheet_name='Sentiment Summary', index=False)\n",
    "                \n",
    "                # Create visualizations for each lyric\n",
    "                create_visualizations(line_df, artist, title, output_folder)\n",
    "                \n",
    "                print(f\"Analysis complete for {artist} - {title}. Results exported to {output_file}\")\n",
    "            \n",
    "            except Exception as e:\n",
    "                logging.error(f\"Error processing dataframe {i}: {e}\")\n",
    "                print(f\"Error processing dataframe {i}: {e}\")\n",
    "    else:\n",
    "        logging.warning(\"No results to export.\")\n",
    "\n",
    "def create_visualizations(df, artist, title, output_folder):\n",
    "    \"\"\"\n",
    "    Create comprehensive visualizations for each lyric\n",
    "    \"\"\"\n",
    "    # Protect against empty DataFrame\n",
    "    if df.empty:\n",
    "        logging.warning(f\"Cannot create visualizations for empty DataFrame: {artist} - {title}\")\n",
    "        return\n",
    "    \n",
    "    # Create a PDF with multiple visualizations\n",
    "    from matplotlib.backends.backend_pdf import PdfPages\n",
    "    \n",
    "    # Output filename\n",
    "    output_filename = os.path.join(output_folder, f'{artist}-{title}_sentiment_visualizations.pdf')\n",
    "    \n",
    "    with PdfPages(output_filename) as pdf:\n",
    "        # 1. Overall Sentiment Distribution\n",
    "        plt.figure(figsize=(15, 5))\n",
    "        plt.subplot(1, 3, 1)\n",
    "        sentiment_counts = df['sentiment'].value_counts()\n",
    "        plt.pie(sentiment_counts, labels=sentiment_counts.index, autopct='%1.1f%%')\n",
    "        plt.title(f'Sentiment Distribution\\n{artist} - {title}')\n",
    "        \n",
    "        # 2. Sentiment Score Distribution\n",
    "        plt.subplot(1, 3, 2)\n",
    "        sns.boxplot(x='sentiment', y='sentiment_score', data=df)\n",
    "        plt.title('Sentiment Scores Distribution')\n",
    "        \n",
    "        # 3. Word Distribution\n",
    "        plt.subplot(1, 3, 3)\n",
    "        plt.bar(['Positive Words', 'Negative Words'], \n",
    "                [df['positive_words'].sum(), df['negative_words'].sum()])\n",
    "        plt.title('Positive vs Negative Words')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        pdf.savefig()\n",
    "        plt.close()\n",
    "        \n",
    "        # Additional visualizations\n",
    "        # Sentiment Score Violin Plot\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        sns.violinplot(x='sentiment', y='sentiment_score', data=df)\n",
    "        plt.title(f'Sentiment Score Distribution\\n{artist} - {title}')\n",
    "        pdf.savefig()\n",
    "        plt.close()\n",
    "\n",
    "def main():\n",
    "    # Perform bulk sentiment analysis\n",
    "    perform_bulk_sentiment_analysis()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d72aa70-9707-4f4c-9b78-acfdcb2f9aa6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
